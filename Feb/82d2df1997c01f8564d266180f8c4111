Apple iPhone X on office desk with social media icons [Courtesy]  A decade and a half ago, Facebook and Twitter were viewed as game changers that enhanced freedom of expression and access to information and political participation.  World events such as the Arab Spring and the recent Sudan Revolution relied on social media for online solidarity and mobilisation. However, it has since emerged that these platforms are cesspools of misinformation, online harassment, bullying, hate speech, and terrorism recruitment.  They have been linked to undermining democracies in the USA, UK, Kenya, and Nigeria, fanning genocide of Rohingya Muslims in Myanmar and fanning ethnic tension in Ethiopia. The bullying and harassment have caused a rise in negative body image and self-esteem issues and suicide and self-harm, especially among young girls and minorities.  After many scandals, incidents, and studies regarding how social media platforms negatively affect societies, they have been pressured to deploy measures to counter problematic speech and practices on their platforms. First, each company has formulated its own standards regarding what is allowed. They police content using complex computer programmes, algorithms, artificial intelligence to flag inappropriate content. Finally, they use human content moderators to review and remove content at the tail end. Notably, African countries disproportionately rely on social media platforms as their primary medium of access to the internet. As such, the actions of these platforms have far-reaching implications on freedom of expression and access to information.  However, platforms have been criticised for not sufficiently investing in regions like Africa and Asia. As a result, the systems do not adequately recognise and flag inappropriate content due to inadequate understanding of local languages and cultural and political contexts, thus fuelling hate speech and other problematic speech.  Recently, Time magazine covered the plight of content moderators from different African countries working in Kenya for Sama, a California based company sub-contracted by Facebook. The story revealed alleged labour malpractices, including poor pay, preventing the workers from unionising contrary to labour rights and poor working conditions.  Kenyans are paid Sh30,000 whilst foreigners are paid Sh50,000. One former manager justified the figures by claiming they did not want to distort the local market. The moderators have complained of mental health issues due to the kind of content they review, including videos and images of murders, rapes, suicides, and child sexual abuse. In addition, the workers allege the company incentivises them to spend less time reviewing longer videos, making it difficult to adequately review situations such as the Ethiopian conflict where hate speech is fuelling targeted killings.  Critics of Facebook contend that moderators not directly working for them allows them to have their cake and eat it. It enables them to claim that they are doing something about the safety of their platforms while violating the rights of African workers who are paid significantly less than their global counterparts.  Because the entities are private, there is very little the government can do regarding pay if they abide by minimum wage requirements. However, working conditions and union-busting is actionable in national and other courts.  By Editorial | 14 hours ago By Ken Gichinga | 14 hours ago By Editorial | 14 hours ago By George Nyatundo | 22 hours ago